---
layout: post
title: "Google三大论文读后感"
description: "Google三大论文读后感"
category: gfs
tags: [gfs,read]
---

最近又看了一遍google三大论文，好像又看懂了不少。在此整理一下，并不是完全系统的理论解释，只是记录下比较有意思的东西。详情还是参考那三大论文吧。

## BigTable ##

**列族**

传统数据库表都是行和列，bigtable里却是行和“列族”，列族是表的chema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。列族就是一坨列。  

具体可以参考[http://www.alidata.org/archives/1509](http://www.alidata.org/archives/1509 "http://www.alidata.org/archives/1509")


列族是访问控制的基本单位。这样对于相同类型的数据列处理起来就集中点了吧，我想这个是提出列族的意义吧。

**时间戳**

Bigtable里的数据是有版本的，区分不同版本就靠时间戳。


**压缩**

为了节省空间，很多时候都采用了压缩的策略。有点拿时间换空间的味道。不过文章中的压缩比确实惊人，说能达10:1。因为重复的文本很多吧：比如相同前缀的URL，不同版本数据间的差异并不大。（压缩这个概念一直很陌生，总感觉是个费时的东西，不应该出现在数据库中，但是想想确实有用。）


**客户端直接和Table通信**

交互的成本是很高的。客户端通过缓存Tablet地址信息直接和Tablet服务器通信，而不是每次都向chubby请求Tablet地址（Tablet地址信息是存储在chubby这个组件中的），bigtable的Master 只负责Tablet服务器的管理与负载。

**落地（compactions）**

Bigtable中的数据不是完全持久化在GFS上的，持久化在GFS上的称之为SSTable（“只读”），还有一部分数据在内存中（memtable），数据操作首先提交在memtable中，等到memtable达到一定量，就落地成SSTable保存在GFS中。当然读取数据的时候是结合memtable和SSTable进行查找读取。对于被标记为删除的数据，是由另外的程序定时循环遍历各个SSTable进行清理的。SSTable只读的特性对于同步访问就很简单了。

**Bloom过滤器**

呵呵，简单确很管用的东西，具体什么东西直接google吧。

**Commit日志的排序**

Commit日志并不陌生，bigtable有两个特殊之处：1、写日志时是一个tablet服务器上追加式的写一个文件（tablet服务器上可能会有很多tablet）。2、在使用commit日志恢复的时候，

先把commit日志进行按tablet排序，这样其他tablet服务器就能很快的定位到自己需要恢复的哪些tablet的位置了。

## GFS ##

GFS的应用环境特性是：

1、存储节点的失效是正常的，容易发生的。

2、数据存储量极为巨大。

3、数据操作通常是尾部追加而不是随机覆盖。对于这个特性我之前一直很不理解。后来看这篇文章**《如何“打败”CAP定理》**[http://blog.fangjian.me/posts/2011/10/18/how-to-beat-the-cap-theorem/](http://blog.fangjian.me/posts/2011/10/18/how-to-beat-the-cap-theorem/ "http://blog.fangjian.me/posts/2011/10/18/how-to-beat-the-cap-theorem/")

其中一个主要观点是：将对数据的操作CRUD就变成了CR（注：CRUD是指Create Read Update Delete，即数据的创建，读取，更新和删除）。详细的可以参看“数据”那一小节。如此一来就很容易理解GFS中提出的“大部分是追加操作，更新操作较少”。GFS也是追加操作性能很好，随机更新操作性能不怎么样了。

GFS的几个构件是：客户端，Master，Chunk服务器。当然，数据交互也是直接和chunkserver交互。

**一次请求会返回额外多的数据**

这样的设计在几篇论文中都有出现，比如客户端向Master请求一个chunk的地址，Master会将这个chunk随后的几个chunk的地址也一起发过去。这也是顺序访问的优化点。**交互成本是较高的**

**Master并不持久化保持chunk的地址信息**

只有chunk server真正知道数据在哪！Master和chunk server直接通过定期握手来交互信息。

**CheckPoint**

CheckPoint文件以B-数的形式数据存储，可以直接映射到内存。无需额外解析。

**租约机制**

起初我以为租约是client和chunk之间的租约。其实它是主chunk和Master之间的租约。

另外多个chunk副本的数据流顺序，这个顺序在客户端操作之初就定好了。

客户端写数据分两部分：1、将数据以任意顺序推送到所有chunk副本上。2、推送没有问题后，客户端再发出控制指令让刚才推送的数据以某种次序进行写入。（数据流和控制流是分开的）

**容错机制**

不管Master服务器和Chunk服务器是如何关闭的，都能快速恢复，这个理念很不错，其实并不需要知道是正常还是异常关闭，总之要恢复过来就是。

**Checksum保证完整性**

这里面提到各个副本之间并不完全一样，所以各个chunkserver必须维护自己的checksum进行校验。Checksum是增量更新的。

## MapReduce ##

**备用任务**

这感觉和木桶原理差不多。最后几个慢的任务会拖延总运行时间。于是在最后快要结束的时候，使用备用任务同时处理剩下的任务。


疑问：

（1）GFS是如何保证网络访问的性能的呢？除了一次访问会额外多获取一些数据。还有其他方式么？

（2）外层的那种SQL语法支持是如何做的，对于多表的关联？

PS:周末把《hadoop源码分析》翻看了一遍，走马观花式的，还是要跟着源码好好研究下。捞起许久不碰的JAVA，好像有那么一份欣喜。